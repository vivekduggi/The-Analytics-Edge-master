Understanding why people vote
===========

###1. Loading data

```{r}
people = read.csv("gerber.csv")
str(people)
prop.table(table(people$voting))
```
```{r}
tapply(people$voting, people$civicduty, mean)
tapply(people$voting, people$hawthorne, mean)
tapply(people$voting, people$self, mean)
tapply(people$voting, people$neighbors, mean)
```
*Note: "Neighbours" treatment group has the largest number of people who voted.


###2. Building logistic regression model

```{r}
peopleLog = glm(voting ~ civicduty + hawthorne + self + neighbors, data = people, family = "binomial")
summary(peopleLog)
```
*Note: In above logistic regression model we see that all our variables "civicduty", "hawthorne", "self", "neighbors" are significant.


###3. Making predictions

1. threshold = 0.3
```{r}
peoplePred = predict(peopleLog, type = "response")
table(people$voting, peoplePred > 0.3)
```
```{r}
cat('Accuracy of our model is =>', (134513 + 51966)/(nrow(people)))
```

2. threshold = 0.5

```{r}
table(people$voting, peoplePred > 0.5)
cat('Accuracy of our model =>', 235388/(235388+108696))
```

3. Computing AUC

```{r}
library(ROCR)
ROCRpred = prediction(peoplePred, people$voting)
as.numeric(performance(ROCRpred, "auc")@y.values)
```

*Note: In our logistic regression model all our variables are significant but is does not improve over our baseline model of predicting
*that no one votes(accuracy = 0.6841004) and our auc is also very low.


###4. Building a CART tree

1. First CART model

*Note: We'll not set the option method = "class", as we would like CART to split our groups if they have different probabilities of voting
*, if we use method = "class", CART would only split if one of the groups had a probability of voting above 50% and other had a probability
*less than 50%.

```{r}
library(rpart)
library(rpart.plot)
CARTmodel = rpart(voting ~ civicduty + hawthorne + self + neighbors, data = people)
prp(CARTmodel)
```


*Note: From above plot we see that no variable makes a big impact on our tree.



2. Second CART model

```{r}
CARTmodel2 = rpart(voting ~ civicduty + hawthorne + self + neighbors, data = people, cp = 0.0)
prp(CARTmodel2)
```



*Note: Our tree detects the trend that group "neighbors" has the highest fraction of people who voted.


3. Third CART model

```{r}
CARTmodel3 = rpart(voting ~ civicduty + hawthorne + self + neighbors + sex, data = people, cp=0.0)
prp(CARTmodel3)
```

4. fourth CART model

This regression tree only uses "control" as an independent variable.
```{r}
CARTmodel4 = rpart(voting ~ control, data = people, cp=0.0)
prp(CARTmodel4, digits = 6)
cat('Absolute difference between voting probability for in control group and other group =>', abs(0.34-0.296638))
```

5. Fifth model

This model uses "control" and "sex" as independent variable.

```{r}
CARTmodel5 = rpart(voting ~ control + sex, data = people, cp = 0.0)
prp(CARTmodel5, digits = 6)
```

###5. Building logistic regression model using "sex" and "control" as independent variables.

```{r}
peopleLog1 = glm(voting ~ control + sex, data = people, family = binomial)
summary(peopleLog1)
```


###Note: The regression tree calculated the percentage voting exactly for every one of the four possibilities (Man, Not Control), (Man, 
###Control), (Woman, Not Control), (Woman, Control). Logistic regression has attempted to do the same, although it wasn't able to do as well because it can't consider exactly the joint possibility of being a women and in the control group.

```{r}
Possibilities = data.frame(sex = c(0,0,1,1), control = c(0,1,0,1))
predict(peopleLog1, newdata = Possibilities, type="response")
```
*Note: The four values in the results correspond to the four possibilities in the order they are stated above ( (Man, Not Control), (Man, Control), (Woman, Not Control), (Woman, Control) ).


###6. Building logistic regression model with independent variables "sex", "control", and a new variable that is the combination of sex
###and control. 

```{r}
peopleLog2 = glm(voting ~ sex + control + sex:control, data = people, family = "binomial")
summary(peopleLog2)
predict(peopleLog2, newdata = Possibilities, type = "response")
```



###We should not use all possible interaction terms in a logistic regression model due to overfitting. Even in this simple problem, we have four treatment groups and two values for sex. If we have an interaction term for every treatment variable with sex, we will double the number of variables. In smaller data sets, this could quickly lead to overfitting.